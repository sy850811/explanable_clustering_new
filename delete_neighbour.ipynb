{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running on CoLab\n",
      "Using mps device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/baqia/miniforge3/envs/torch-gpu/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#extract neighGraph for concepts of clusters miltithreaded\n",
    "import requests,os\n",
    "import threading\n",
    "import urllib\n",
    "import time\n",
    "import json\n",
    "import importlib\n",
    "import pickle\n",
    "load = importlib.import_module(\"loadStuff\")\n",
    "cluster_neighbour = importlib.import_module(\"cluster_neighbour\")\n",
    "env = importlib.import_module(\"envf\")\n",
    "root_address = env.root_address\n",
    "results = [[],[],[],[]]\n",
    "counter=0\n",
    "no_of_threads_already_running = 0\n",
    "# Limit the number of threads.\n",
    "pool = threading.BoundedSemaphore(50)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locally_fetch_dataset_concepts():\n",
    "    filename = os.path.join(root_address +env.DATASET_NAME+ \"/annotation.json\")\n",
    "    # Ensure the file exists before trying to open it\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, 'r') as file:\n",
    "            data = json.load(file)\n",
    "            \n",
    "            # Initialize a list to hold all the strings within the lists in all the values\n",
    "            dataset_concepts = []\n",
    "            \n",
    "            # Iterate over the dictionary and extract all strings from the lists in the values\n",
    "            for value in data.values():\n",
    "                if isinstance(value, list):\n",
    "                    for item in value:\n",
    "                        if isinstance(item, str):\n",
    "                            dataset_concepts.append(item)\n",
    "    else:\n",
    "        dataset_concepts = None\n",
    "\n",
    "    return list(set(dataset_concepts))\n",
    "def make_url_list(concepts):\n",
    "    urls = []\n",
    "    for concept in concepts:\n",
    "        data = urllib.parse.urlencode([(\"lang\", 'en'), (\"title\", concept),\n",
    "            (\"nPredLevels\", 0), (\"nSuccLevels\", 1)])\n",
    "        urls.append(\"http://www.wikifier.org/get-neigh-graph?\" + data)\n",
    "    return urls\n",
    "\n",
    "def worker(u,concept_neighbour_local_dict,v):\n",
    "    # Request passed URL.\n",
    "    while True:\n",
    "        try:\n",
    "            r = requests.get(u)\n",
    "            if r.status_code == 200:\n",
    "                global counter\n",
    "                counter+=1\n",
    "                print(\"success\"+ str(counter))\n",
    "                concept_neighbour_local_dict[v] = r.text\n",
    "                pool.release()\n",
    "                return\n",
    "            else:\n",
    "                print(\"error\")\n",
    "                time.sleep(2)\n",
    "        except:\n",
    "            print(\"failure\")\n",
    "\n",
    "\n",
    "\n",
    "def req(concepts):\n",
    "    concept_neighbour_local_dict = {}\n",
    "    # Get URLs from a text file, remove white space.\n",
    "    urls = make_url_list(concepts)\n",
    "    no_of_threads_already_running = threading.active_count()\n",
    "    for u,v in zip(urls,concepts):\n",
    "        # Thread pool.\n",
    "        # Blocks other threads (more than the set limit).\n",
    "        pool.acquire(blocking=True)\n",
    "        # Create a new thread.\n",
    "        # Pass each URL (i.e. u parameter) to the worker function.\n",
    "        t = threading.Thread(target=worker, args=(u,concept_neighbour_local_dict,v))\n",
    "        # Start the newly create thread.\n",
    "        t.start()\n",
    "\n",
    "    #wait for all threads to finish\n",
    "    while threading.active_count() > no_of_threads_already_running:\n",
    "        print(\"waiting\",threading.active_count())\n",
    "        time.sleep(2)\n",
    "        \n",
    "    print(\"done\")\n",
    "    with open(root_address + env.DATASET_NAME +\"/\"+ \"local_concept_neighbour.json\", \"w\") as write_file:\n",
    "        json.dump(concept_neighbour_local_dict, write_file)\n",
    "\n",
    "def extract_neighbourhood_for_each_cluster():\n",
    "    unique_concepts = load.unique_concepts_for_each_cluster()\n",
    "    for i in range(len(unique_concepts)):\n",
    "        try:\n",
    "            req(i,unique_concepts[i])\n",
    "        except:\n",
    "            print(\"error in cluster \",i)\n",
    "\n",
    "def extract_neighbourhood_for_dataset():\n",
    "    dataset_concepts = locally_fetch_dataset_concepts()\n",
    "    concept_neighbour_local_dict = req(dataset_concepts)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_neighbourhood_for_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the local_concept_neighbour.json file\n",
    "\n",
    "\n",
    "\n",
    "with open(root_address + env.DATASET_NAME +\"/\"+ \"local_concept_neighbour.json\", \"r\") as read_file:\n",
    "    concept_neighbour_local_dict = json.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_neighbourhood_for_each_cluster():\n",
    "    unique_concepts = load.unique_concepts_for_each_cluster()\n",
    "    with open(root_address + env.DATASET_NAME +\"/\"+ \"local_concept_neighbour.json\", \"r\") as read_file:\n",
    "        concept_neighbour_local_dict = json.load(read_file)\n",
    "    results = [[] for i in range(len(unique_concepts))]\n",
    "    for i in range(len(unique_concepts)):\n",
    "        for j in range(len(unique_concepts[i])):\n",
    "            try:\n",
    "\n",
    "                results[i].append({unique_concepts[i][j]:concept_neighbour_local_dict[unique_concepts[i][j]]})\n",
    "            except:\n",
    "                print(\"error in cluster \",i)\n",
    "        with open(root_address + env.DATASET_NAME +\"/\"+ \"cluster\"+str(i)+\"_neighbour.json\", \"w\") as write_file:\n",
    "            json.dump(results[i], write_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_neighbourhood_for_each_cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
